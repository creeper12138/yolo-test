# ğŸ¯ YOLO è‡ªåŠ¨ç„å‡†ç³»ç»Ÿï¼ˆè‡ªç„è¾…åŠ©ï¼‰

æœ¬é¡¹ç›®æ˜¯ä¸€ä¸ªåŸºäº [Ultralytics YOLOv8](https://github.com/ultralytics/ultralytics) å’Œ `dxcam` çš„å®æ—¶ç›®æ ‡æ£€æµ‹ä¸è‡ªç„è¾…åŠ©ç³»ç»Ÿï¼Œæ”¯æŒé€šè¿‡é”®ç›˜æ§åˆ¶å¼€å¯/å…³é—­ä»¥åŠé€€å‡ºç¨‹åºã€‚é€‚ç”¨äºéœ€è¦é«˜å¸§ç‡ã€ä½å»¶è¿Ÿçš„è§†è§‰æ£€æµ‹ä»»åŠ¡ï¼Œå¦‚æ¸¸æˆè¾…åŠ©ã€è‡ªå®šä¹‰å§¿æ€è¯†åˆ«åº”ç”¨ç­‰ã€‚

## âœ¨ åŠŸèƒ½æ¦‚è§ˆ

* ğŸ” **ç›®æ ‡æ£€æµ‹**ï¼šä½¿ç”¨ YOLOv8 pose æ¨¡å‹ (`.engine`) è¿›è¡Œäººä½“å…³é”®ç‚¹æ£€æµ‹
* ğŸ¯ **è‡ªåŠ¨ç„å‡†**ï¼šå¯¹è·ç¦»å±å¹•ä¸­å¿ƒæœ€è¿‘çš„ç›®æ ‡è¿›è¡Œè¿½è¸ªå¹¶ç§»åŠ¨é¼ æ ‡æŒ‡é’ˆ
* ğŸ–± **è‡ªåŠ¨ç‚¹å‡»**ï¼šå½“ç›®æ ‡è¿›å…¥é¢„è®¾åŒºåŸŸèŒƒå›´å†…è‡ªåŠ¨æ‰§è¡Œç‚¹å‡»æ“ä½œ
* ğŸ› **PID-like å¹³æ»‘æ§åˆ¶**ï¼šç¼“è§£é«˜é¢‘æ£€æµ‹å¯¼è‡´çš„é¼ æ ‡æŠ–åŠ¨ï¼Œå®ç°æ›´å¹³æ»‘çš„ç§»åŠ¨
* âŒ¨ï¸ **å¿«æ·é”®æ§åˆ¶**ï¼š

  * `Home`ï¼šå¯åŠ¨/å…³é—­è‡ªç„
  * `End`ï¼šé€€å‡ºç¨‹åº

## ğŸ“· æ£€æµ‹åŒºåŸŸè®¾ç½®

é€šè¿‡ `dxcam` é‡‡é›†éƒ¨åˆ†å±å¹•åŒºåŸŸï¼ˆé»˜è®¤åˆ†è¾¨ç‡ä¸º 2560x1440ï¼‰ï¼š

```python
region = (1000, 650, 1560, 1000)  # å·¦ä¸Šè§’ (x, y), å³ä¸‹è§’ (x, y)
```

## ğŸ”§ ä¾èµ–å®‰è£…

```bash
pip install -r requirements.txt
```

`requirements.txt` å†…å®¹å‚è€ƒï¼š

```txt
ultralytics
dxcam
opencv-python
pynput
torch
numpy
```

æ­¤å¤–ï¼Œéœ€è¦å®‰è£… DirectX Runtime ä»¥ç¡®ä¿ `dxcam` æ­£å¸¸å·¥ä½œã€‚

## ğŸš€ ä½¿ç”¨æ–¹å¼

1. æ›¿æ¢æ¨¡å‹æ–‡ä»¶ï¼š
   å°†ä½ è‡ªå·±çš„ YOLOv8 TensorRT å¼•æ“æ¨¡å‹æ”¾å…¥é¡¹ç›®ç›®å½•ä¸­ï¼Œå¹¶å‘½åä¸º `yolo11n-pose.engine`ï¼Œæˆ–ä¿®æ”¹ä»£ç ä¸­æ¨¡å‹è·¯å¾„ã€‚

2. å¯åŠ¨ç¨‹åºï¼š

   ```bash
   python main.py
   ```

3. æ“ä½œæ§åˆ¶ï¼š

   * æŒ‰ä¸‹ `Home` é”®å¯åŠ¨æˆ–æš‚åœè‡ªåŠ¨æ£€æµ‹
   * æŒ‰ä¸‹ `End` é”®ç»ˆæ­¢ç¨‹åº

## ğŸ§  å·¥ä½œåŸç†ç®€è¦è¯´æ˜

* ä½¿ç”¨ `dxcam` æ•è·è®¾å®šå±å¹•åŒºåŸŸå›¾åƒï¼›
* é€šè¿‡ YOLOv8 æ¨¡å‹è¿›è¡Œäººä½“å§¿æ€æ£€æµ‹ï¼›
* é€‰å–è·ç¦»å±å¹•ä¸­å¿ƒæœ€è¿‘çš„ç›®æ ‡ï¼›
* è®¡ç®—å…¶ä½ç½®åç§»é‡ï¼Œä½¿ç”¨â€œæƒ¯æ€§/å·®åˆ†â€æ–¹å¼æ§åˆ¶é¼ æ ‡å¹³æ»‘ç§»åŠ¨ï¼›
* æ£€æµ‹åˆ°ç›®æ ‡è¿›å…¥ä¸­å¤®åŒºåŸŸæ—¶è§¦å‘ç‚¹å‡»æ“ä½œã€‚

## ğŸ”’ å®‰å…¨ä¸åˆæ³•æ€§å£°æ˜

æœ¬é¡¹ç›®ä»…ç”¨äºå­¦ä¹ ä¸ç ”ç©¶ç›®çš„ï¼Œ**ç¦æ­¢ç”¨äºä»»ä½•è¿åå¹³å°æˆ–æ¸¸æˆè§„åˆ™çš„è¡Œä¸º**ã€‚ä½œè€…ä¸å¯¹ä»»ä½•æ»¥ç”¨é€ æˆçš„åæœæ‰¿æ‹…è´£ä»»ã€‚

## ğŸ“ é¡¹ç›®ç»“æ„ï¼ˆå…³é”®éƒ¨åˆ†ï¼‰

```
â”œâ”€â”€ main.py               # ä¸»ç¨‹åºé€»è¾‘
â”œâ”€â”€ PID.py                # é¼ æ ‡æ§åˆ¶ä¸ç‚¹å‡»é€»è¾‘æ¨¡å—
â”œâ”€â”€ yolo11n-pose.engine   # TensorRT ç¼–è¯‘åçš„ YOLOv8 æ¨¡å‹æ–‡ä»¶
â””â”€â”€ README.md             # é¡¹ç›®è¯´æ˜
```

## ğŸ’¬ å‚è€ƒç»„ä»¶

* [Ultralytics YOLOv8](https://github.com/ultralytics/ultralytics)
* [dxcam](https://github.com/Prayag2/dxcam)
* [pynput](https://pynput.readthedocs.io/en/latest/)
* [Torch (CUDA)](https://pytorch.org/)

---
å“ˆä¸Šé¢éƒ½æ˜¯AIå†™çš„ï¼Œè¿™ä¸ªä¸œè¥¿æ˜¯å‚è€ƒhttps://blog.csdn.net/mrathena/article/details/126860226 åšçš„ 1ts.pyæ˜¯ä¸»è¦ç‰ˆæœ¬ 2ts.pyæ˜¯ä¸€ä¸ªè”ç½‘ç‰ˆæœ¬ï¼Œå°±æ˜¯è®¡ç®—æ”¾åœ¨å…¶ä»–ç”µè„‘ä¸Š ï¼Œ3ts.pyå’Œ1å·®ä¸å¤šï¼Œåªä¸è¿‡æ˜¯ç›´æ¥é”å¤´çš„ï¼Œä½†æ˜¯éœ€è¦è®¡ç®—çš„å°±æ›´å¤šï¼Œ1é‡Œé¢é”èº«ä½“ç”¨ç®—æ³•æ¥é—´æ¥æ‰€å¤´
S.PY æ˜¯å®¢æˆ·ç«¯ï¼ŒT.PYæ˜¯æœåŠ¡å™¨ç«¯ ä½ è¯´æ€ä¹ˆæ²¡æœ‰t.py æ”¾æœåŠ¡å™¨ä¸Šäº† åæ­£å°±æ˜¯æŠŠæ¨¡å‹è¿ç®—çš„éƒ¨åˆ†æ‹†å‡ºæ¥ å¤§æ¦‚å°±æ˜¯è¿™æ · æˆ‘çš„é…ç½®æ˜¯4060çš„ç¬”è®°æœ¬ æ¯ä¸ªå¾ªç¯è€—æ—¶å¤§æ¦‚åœ¨30-40msã€‚è¿˜æœ‰ä»€ä¹ˆé—®é¢˜å‘¢ï¼Œåœ¨dxcam.py é‡Œé¢
 if frame is not None:
                    with self.__lock:
                        # Reconstruct frame buffer when resolution change
                        if frame.shape[0] != self.height:
                            region = (1000,650, 1560, 1000)
                            frame_shape = (region[3] - region[1], region[2] - region[0], self.channel_size)
                            self.__frame_buffer = np.ndarray(
                                (self.max_buffer_len, *frame_shape), dtype=np.uint8
                            )
                        self.__frame_buffer[self.__head] = frame
è¿™ä¸ªåœ°æ–¹è¿™ä¹ˆæ”¹ï¼Œä¸ç„¶ä¼šæŠ¥é”™ï¼Œæˆ‘è®°å¾—æ˜¯å¸§æº¢å‡ºï¼Ÿæ”¹äº†æœ‰æ—¶å€™ä¹Ÿä¼šæŠ¥é”™ï¼Œä¸çŸ¥é“ä¸ºä»€ä¹ˆï¼ŒçŸ¥é“çš„å¯ä»¥è¯´è¯´
PIDé»˜è®¤æ²¡æœ‰å¼€ï¼Œæ•´ä¸ªè¿‡ç¨‹è€—æ—¶é—´æœ€å¤šçš„æ˜¯å…·ä½“ç§»åŠ¨é¼ æ ‡çš„éƒ¨åˆ†ï¼Œ
è™½ç„¶æˆ‘æ˜¯èœé¸Ÿï¼Œä½†æ˜¯è¿˜ç®—æ˜¯è§£å†³äº†ä¸€ä¸ªé—®é¢˜, æˆ‘å‚è€ƒçš„åšä¸»åœ¨æœ€åè¯´ä¼šäº§ç”Ÿéœ‡è¡ç°è±¡ï¼Œä½†æˆ‘æ¨è¿ŸåŸå› æ˜¯ç¨‹åºçš„é©±åŠ¨å‡½æ•°moveå®é™…ä¸Šå‘ç»™é©±åŠ¨ä¸€ä¸ªä¿¡å·ï¼Œç„¶åé©±åŠ¨æ§åˆ¶é¼ æ ‡ç§»åŠ¨ï¼Œå¹³å¸¸è¿™ä¸ªç§»åŠ¨æ—¶é—´å¾ˆå°å¯ä»¥å¿½ç•¥ï¼Œä½†æ˜¯å¦‚æœè¿è¡Œçš„é€Ÿåº¦å¤Ÿå¿«ï¼Œå°±ä¼šå¯¼è‡´åœ¨é¼ æ ‡ä¸Šä¸€æ¬¡å‘½ä»¤æ²¡æœ‰æ‰§è¡Œå®Œï¼Œè¿˜åœ¨ç§»åŠ¨çš„è¿‡ç¨‹ä¸­ï¼Œç¨‹åºåˆä¸€æ¬¡æ¥åˆ°è¿™ä¸ªåœ°æ–¹ï¼Œç„¶åè®¡ç®—ç›®æ ‡ä½ç½®ï¼Œå†æ¬¡å‘å‡ºæŒ‡ä»¤ã€‚è¿™é‡Œæ˜¯å¼‚æ­¥çš„ï¼Œé¼ æ ‡é©±åŠ¨çš„é€»è¾‘åº”è¯¥æ˜¯é˜Ÿåˆ—æŒ‡ä»¤ï¼Œæœ€ç»ˆå¯¼è‡´åŸæœ¬æ˜¯Aç§»åŠ¨åˆ°Bï¼Œå˜æˆA-C-Bçš„æŒ‡ä»¤ï¼Œç¬¬ä¸€ä¸ªæŒ‡ä»¤A-BåŠ ä¸ŠC-Bçš„è·ç¦»ä¹Ÿå°±è¶…è¿‡äº†A-Bçš„åŸå§‹è·¯ç¨‹ï¼Œæ€§èƒ½è¶Šå¥½é€Ÿåº¦è¶Šå¿«éœ‡è¡ä¹Ÿå°±è¶Šä¸¥é‡
è¿™é‡Œçš„ä»£ç å°±æ˜¯è§£å†³è¿™ä¸ªé—®é¢˜çš„ï¼Œä¸æ˜¯å®Œç¾è§£å†³ï¼Œè¿™ä¸œè¥¿ä¼šæœ‰å¤šå°‘äººçœ‹å‘¢
                    p = lasx - x
                    lasx = x
                    err = lasmove_x - p
                    lasmove_x = move_x
                    if -10 < p < 10:
                        err = err *0.65
                    if move_x >= 0 and err > 0:
                        move_x = move_x - err * 0.8
                    elif move_x < 0 and err > 0:
                        move_x = move_x + err * 0.8
                    elif move_x > 0 and err < 0:
                        move_x = move_x + err * 0.8
                    elif move_x < 0 and err < 0:
                        move_x = move_x - err * 0.8
                    if 0 < move_x< 1:
                        move_x = 1
                    elif 0 > move_x > -1:  # é™åˆ¶è´Ÿæœ€å°å€¼
                        move_x = -1

Here's a full English version of your `README.md`, including both the polished formal documentation and your detailed developer notes, preserving all your technical insight:

---

# ğŸ¯ YOLO Auto-Aim System (Aimbot Assistant)

This project is a real-time auto-aiming assistant built on [Ultralytics YOLOv8](https://github.com/ultralytics/ultralytics) and `dxcam`, supporting keyboard control to toggle detection or exit the program. It is designed for high-FPS, low-latency scenarios like game assistance or custom pose detection.

## âœ¨ Features Overview

* ğŸ” **Object Detection**: Uses YOLOv8 pose model (`.engine`) for human keypoint detection
* ğŸ¯ **Auto Aim**: Tracks the closest target to the screen center and moves the mouse pointer accordingly
* ğŸ–± **Auto Click**: Automatically triggers mouse clicks when the target enters a specified region
* ğŸ› **PID-like Smooth Control**: Mitigates jitter from high-frequency detections, ensures smoother motion
* âŒ¨ï¸ **Keyboard Shortcuts**:

  * `Home`: Toggle auto-detection on/off
  * `End`: Exit the program

## ğŸ“· Screen Capture Region

Uses `dxcam` to capture a portion of the screen (default for 2560Ã—1440 resolution):

```python
region = (1000, 650, 1560, 1000)  # Top-left (x, y) to bottom-right (x, y)
```

## ğŸ”§ Dependencies

```bash
pip install -r requirements.txt
```

Sample `requirements.txt`:

```txt
ultralytics
dxcam
opencv-python
pynput
torch
numpy
```

Also make sure [DirectX Runtime](https://www.microsoft.com/en-us/download/details.aspx?id=35) is installed for `dxcam` to work correctly.

## ğŸš€ How to Use

1. **Add your model**:
   Put your custom YOLOv8 TensorRT engine file in the project root, named `yolo11n-pose.engine`. Modify the path in `main.py` if using a different name.

2. **Run the program**:

   ```bash
   python main.py
   ```

3. **Control**:

   * Press `Home` to toggle detection
   * Press `End` to stop the program

## ğŸ§  Behind the Scenes

* Uses `dxcam` to continuously grab frames from a defined screen region
* Feeds images into YOLOv8 pose model for human keypoint detection
* Selects the closest target to screen center
* Calculates movement offsets and applies smoothed movement via mouse controller
* Triggers clicks when the target is within a defined "headshot zone"

## ğŸ§ª Developer Notes & Tips

This project is heavily inspired by [this blog post (CSDN)](https://blog.csdn.net/mrathena/article/details/126860226). Here's how my version expands on it:

### ğŸ—ƒ Versions

* `1ts.py`: main version â€“ detects body and indirectly locks head
* `2ts.py`: offloaded (networked) version â€“ sends frames to another PC for inference
* `3ts.py`: similar to `1ts`, but directly locks onto the head (heavier computation)
* `S.py`: client code (for networked version)
* `T.py`: server code (runs on the remote machine with model, not included here for security)

### âš™ Hardware

* GPU: Laptop with RTX 4060
* Inference loop time: \~30â€“40ms per cycle
* Most time-consuming step: moving the mouse (due to system-level driver delay)

### âš  `dxcam.py` Patch

To avoid "frame overflow" errors, I modified the frame shape reset in `dxcam.py` like this:

```python
if frame is not None:
	with self.__lock:
		# Reconstruct frame buffer when resolution change
		if frame.shape[0] != self.height:
			region = (1000,650, 1560, 1000)
			frame_shape = (region[3] - region[1], region[2] - region[0], self.channel_size)
			self.__frame_buffer = np.ndarray(
				(self.max_buffer_len, *frame_shape), dtype=np.uint8
		 )
		self.__frame_buffer[self.__head] = frame
```

This prevents crash in most cases, but it still sometimes fails. Possibly related to buffer size or async grabbing bugs â€“ contributions welcome.

### ğŸ§© Why it Jitters (and How I Solved It)

The original author mentioned â€œoscillationâ€ at high FPS. I discovered it's not just control error â€“ it's because:

> The mouse movement is **asynchronous**. When you send move Aâ†’B, the cursor might still be moving from A when your loop sends another command Câ†’B. The system queues them up (likely in the driver), causing Aâ†’Câ†’B pattern, overshooting your target.

This bug is *worse on faster systems*, because the loop updates more quickly than the mouse can physically move.

I applied a lightweight prediction correction to reduce this:

```python
p = lasx - x
lasx = x
err = lasmove_x - p
lasmove_x = move_x
if -10 < p < 10:
	err = err * 0.65
if move_x >= 0 and err > 0:
	move_x -= err * 0.8
elif move_x < 0 and err > 0:
	move_x += err * 0.8
elif move_x > 0 and err < 0:
	move_x += err * 0.8
elif move_x < 0 and err < 0:
	move_x -= err * 0.8
if 0 < move_x < 1:
	move_x = 1
elif 0 > move_x > -1:
	move_x = -1
```

Itâ€™s not perfect but helps *a lot*. Feel free to improve it.

### ğŸ¤” Who Will Use This?

Maybe only a few hobbyists â€“ but even if you're a beginner like me, solving this kind of practical problem is rewarding.

## ğŸ”’ Legal Disclaimer

This tool is for **educational and research use only**.
**Do not** use it to violate any game rules, platform terms, or ethical boundaries.
The author is not responsible for misuse.

## ğŸ“‚ Project Structure

```
â”œâ”€â”€ main.py               # Core detection logic
â”œâ”€â”€ PID.py                # Mouse movement + click logic
â”œâ”€â”€ yolo11n-pose.engine   # Your TensorRT YOLOv8 model
â”œâ”€â”€ S.py / T.py           # Networked inference version (client/server)
â”œâ”€â”€ 1ts.py / 2ts.py / 3ts.py  # Different versions with slight behavioral tweaks
â””â”€â”€ README.md             # This file
```

## ğŸ’¬ References & Components

* [Ultralytics YOLOv8](https://github.com/ultralytics/ultralytics)
* [dxcam](https://github.com/Prayag2/dxcam)
* [pynput](https://pynput.readthedocs.io/en/latest/)
* [Torch (CUDA)](https://pytorch.org)

---

Let me know if you'd like this split into multiple language versions (`README.md` + `README.zh.md`) or if you'd like an auto-generated `requirements.txt` and `dxcam` patch template.
